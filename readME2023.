"""
I.Contacts:
Contact me at kaloev_92@mail.ru
eng. M. Kaloev

II.how to start
1.Check important versions (this is automaticaly done for cloud version)
2.Press Run Button
3.Collect the activation function signals from the dir: results

III.What code does
1.Creates model of neural network
2.Generates traing data (random data)
3.Model is trained and it weights are saved
4.Signals output of hiden layers are displayed 
4.1 signal of Relu is used for traing
4.2 signal of all other activation function is shown after relu is changed for other functions in longest layer (not trained)


IV.important versions:
keras: 2.6.0
numpy: 1.19.5
tensorflow: 2.6.0
python 3.9

V.model:
Model: "sequential"
____________________________
input shape: (40)
_____________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 60)                2460      
_________________________________________________________________
dense_1 (Dense)              (None, 80)                4880      
_________________________________________________________________
dense_2 (Dense)              (None, 100)               8100      
_________________________________________________________________
dense_3 (Dense)              (None, 75)                7575      
_________________________________________________________________
dense_4 (Dense)              (None, 50)                3800      
_________________________________________________________________
dense_5 (Dense)              (None, 25)                1275      
_________________________________________________________________
dense_6 (Dense)              (None, 5)                 130       
=================================================================
Total params: 28,220
Trainable params: 28,220
Non-trainable params: 0

VI.discussion

Q1. there are huge areas with out put of zeros:
A: yes, this expected and downside of using relu activation

Q2. why leakyRelu is NOT having same output as relu if they are same "famillies":
A: leakyrelu and relu behave in same way only for positives outputs

Q3. sometimes sigmoid can swaped  relu (sigmoid -> relu) in hiden layer with same weight  withot changing results, 
and some time is (tanh-relu), why?
A: Do check the training information for the neutwork, if "loss" is close to 0 and "acc" is 1.000 from the epoch 20+, this may be because the
NN is feed with very similar information and is overspecilised (overftted), in this case the "noise" signal that tanh creates with similar to 
bad signal relu produces in the hiden layers. This effect is discussed in the papper. 

Q4. some functions are causing masive "explosions" of signals bigger than relu?
A: yes, this is to expected, please do read documentation for: exponential activation function

Q5. my system is laging on some actiavtion functions and produces realy bad signal with valuse [-0.3 to 0.3]
A: yes, this is to expected, some activation function are use primary for output layers in NN for clasification
do read documentation for : softmax, softsign

Q6. some signals have very low, negative values?
A: Linear activation function does NOT change input-outputs.

VII.disclaimer
This code and the paper it supports are focused on signals that activation functions provides in hiden layers
Based on the code NO generalised conclusions should be made for the topics as: size of neural networks, overfititing problems, selecting good traing data, learning rate, epochs,
or other parameters for building of NN. 


ps: i am dislexyc , if  mispeleed something.
